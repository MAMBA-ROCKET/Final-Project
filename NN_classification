import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import time
import matplotlib.pyplot as plt

# Define a simple MLP with one hidden layer (using tanh for smooth second derivatives)
class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)
        self.tanh = nn.Tanh()
        
    def forward(self, x):
        x = self.fc1(x)
        x = self.tanh(x)
        x = self.fc2(x)
        return x

# Create a synthetic binary classification dataset
def create_dataset(n_samples=10000, input_dim=20):
    X = np.random.randn(n_samples, input_dim)
    # Create a random linear combination for generating labels
    true_w = np.random.randn(input_dim)
    logits = X.dot(true_w)
    probs = 1 / (1 + np.exp(-logits))
    y = (probs > 0.5).astype(np.float32)
    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).view(-1, 1)

# Utility functions to flatten and set the parameters of the model
def get_flat_params(model):
    return torch.cat([p.view(-1) for p in model.parameters()])

def set_flat_params(model, flat_params):
    offset = 0
    for p in model.parameters():
        numel = p.numel()
        p.data.copy_(flat_params[offset:offset+numel].view_as(p))
        offset += numel

def get_flat_grad(loss, model):
    grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)
    return torch.cat([g.contiguous().view(-1) for g in grads])

# Hessian-vector product using Pearlmutter's method
def hessian_vector_product(model, loss, vector):
    grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)
    flat_grad = torch.cat([g.contiguous().view(-1) for g in grads])
    grad_v = torch.dot(flat_grad, vector)
    Hv = torch.autograd.grad(grad_v, model.parameters(), retain_graph=True)
    return torch.cat([h.contiguous().view(-1) for h in Hv])

# Helper to compute tau so that ||p + tau*d|| = trust_radius
def find_tau(p, d, trust_radius):
    # Solve ||p + tau*d||^2 = trust_radius^2
    p_norm = p.norm().item()
    d_norm = d.norm().item()
    pd = torch.dot(p, d).item()
    rad = trust_radius
    # Coefficients of the quadratic: a*tau^2 + b*tau + c = 0
    a = d_norm**2
    b = 2 * pd
    c = p_norm**2 - rad**2
    discriminant = b**2 - 4*a*c
    if discriminant < 0:
        tau = 0
    else:
        sqrt_disc = np.sqrt(discriminant)
        tau = (-b + sqrt_disc) / (2*a)
        if tau < 0 or tau > 1:
            tau = (-b - sqrt_disc) / (2*a)
    return tau

# Truncated CG method to solve the trust region subproblem: minimize m(p) = g^T p + 0.5 p^T H p subject to ||p|| <= trust_radius
def truncated_cg(model, loss, g, trust_radius, cg_tol=1e-4, max_iter=50):
    p = torch.zeros_like(g)
    r = -g.clone()  # residual = -g - H*p (with p=0 initially)
    d = r.clone()
    if r.norm() < cg_tol:
        return p
    for i in range(max_iter):
        Hd = hessian_vector_product(model, loss, d)
        dHd = torch.dot(d, Hd)
        if dHd <= 0:
            # Negative curvature: step to the boundary
            tau = find_tau(p, d, trust_radius)
            return p + tau * d
        alpha = torch.dot(r, r) / dHd
        p_next = p + alpha * d
        if p_next.norm() >= trust_radius:
            tau = find_tau(p, d, trust_radius)
            return p + tau * d
        r_next = r - alpha * Hd
        if r_next.norm() < cg_tol:
            return p_next
        beta = torch.dot(r_next, r_next) / torch.dot(r, r)
        d = r_next + beta * d
        p = p_next
        r = r_next
    return p

# Trust Region Newton-CG training loop
def train_trust_region(model, X, y, num_iters=50, trust_radius_init=1.0, eta=0.1):
    losses = []
    trust_radius = trust_radius_init
    for i in range(num_iters):
        model.zero_grad()
        outputs = model(X)
        # Using binary cross-entropy with logits loss
        loss = nn.functional.binary_cross_entropy_with_logits(outputs, y)
        loss_val = loss.item()
        losses.append(loss_val)
        print(f"Iteration {i}, loss: {loss_val:.4f}, trust_radius: {trust_radius:.4f}")
        
        # Compute the gradient at the current parameters
        g = get_flat_grad(loss, model)
        if g.norm().item() < 1e-4:
            print("Gradient norm small, stopping.")
            break

        # Use truncated CG to compute the approximate Newton step within the trust region
        p = truncated_cg(model, loss, g, trust_radius)
        
        # Compute predicted reduction: m(0)-m(p) = -g^T p - 0.5 * p^T H p
        Hp = hessian_vector_product(model, loss, p)
        pred_reduction = -torch.dot(g, p) - 0.5 * torch.dot(p, Hp)
        
        # Save current parameters and update with the step
        flat_params_old = get_flat_params(model)
        new_params = flat_params_old + p
        set_flat_params(model, new_params)
        
        # Evaluate the new loss
        outputs_new = model(X)
        loss_new = nn.functional.binary_cross_entropy_with_logits(outputs_new, y)
        actual_reduction = loss.item() - loss_new.item()
        rho = actual_reduction / pred_reduction.item() if pred_reduction.item() != 0 else 0
        
        # Update the trust region radius based on the ratio rho
        if rho < 0.25:
            trust_radius *= 0.25
        elif rho > 0.75 and abs(p.norm().item() - trust_radius) < 1e-6:
            trust_radius = min(2 * trust_radius, 100.0)
        
        # Accept or reject the step
        if rho < eta:
            set_flat_params(model, flat_params_old)
            print("Step rejected")
        else:
            print("Step accepted")
    return losses

# Standard SGD training using PyTorch's optimizer (for comparison)
def train_sgd(model, X, y, num_epochs=50, lr=1e-2):
    optimizer = optim.SGD(model.parameters(), lr=lr)
    losses = []
    for epoch in range(num_epochs):
        optimizer.zero_grad()
        outputs = model(X)
        loss = nn.functional.binary_cross_entropy_with_logits(outputs, y)
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
        print(f"Epoch {epoch}, loss: {loss.item():.4f}")
    return losses

# Main function to run the experiments
def main():
    torch.manual_seed(0)
    np.random.seed(0)
    # Create synthetic dataset with 10,000 samples and 20 features
    X, y = create_dataset(n_samples=10000, input_dim=20)
    
    # Create two identical models for a fair comparison
    hidden_dim = 50  # This ensures more than 1,000 weights overall
    model_trust = MLP(input_dim=20, hidden_dim=hidden_dim, output_dim=1)
    model_sgd = MLP(input_dim=20, hidden_dim=hidden_dim, output_dim=1)
    model_sgd.load_state_dict(model_trust.state_dict())
    
    # Train with Trust Region Newton-CG
    print("Training with Trust Region Newton-CG")
    start_time = time.time()
    losses_trust = train_trust_region(model_trust, X, y, num_iters=30, trust_radius_init=1.0, eta=0.1)
    time_trust = time.time() - start_time
    print("Trust Region Newton-CG completed in {:.2f} seconds".format(time_trust))
    
    # Train with SGD
    print("\nTraining with SGD")
    start_time = time.time()
    losses_sgd = train_sgd(model_sgd, X, y, num_epochs=30, lr=1e-2)
    time_sgd = time.time() - start_time
    print("SGD completed in {:.2f} seconds".format(time_sgd))
    
    # Plot training loss curves
    plt.plot(losses_trust, label="Trust Region Newton-CG")
    plt.plot(losses_sgd, label="SGD")
    plt.xlabel("Iteration/Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.title("Training Loss Comparison")
    plt.show()

if __name__ == '__main__':
    main()